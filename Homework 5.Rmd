---
title: "Homework 5"
author: "Mari Sanders"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(faraway)
library(corrplot)
library(leaps)
library(glmnet)
library(patchwork)
library(caret)
```

a) 

```{r}

state_data <- 
  state.x77 %>% as_tibble() %>% janitor::clean_names()
state_data %>% summary() %>% knitr::kable()
state_data %>% 
  summarize(population_sd = sd(population, na.rm = TRUE),
            income_sd = sd(income, na.rm = TRUE),
            illiteracy_sd = sd(illiteracy, na.rm = TRUE),
            lifeexpec_sd = sd(life_exp, na.rm = TRUE),
            murder_sd = sd(murder, na.rm = TRUE),
            hsgrad_sd = sd(hs_grad, na.rm = TRUE),
            frost_sd = sd(frost, na.rm = TRUE),
            area_sd = sd(area, na.rm = TRUE)) %>% knitr::kable()
```

b) 

```{r}
pop <- ggplot(state_data, aes(x = log(population))) + geom_histogram()

income <- ggplot(state_data, aes(x = income)) + geom_histogram()

ggplot(state_data, aes(x = 1/(log(illiteracy) + 1))) + geom_histogram()

ggplot(state_data, aes(x = life_exp)) + geom_histogram()
 
ggplot(state_data, aes(x = hs_grad)) + geom_histogram()

ggplot(state_data, aes(x = frost)) + geom_histogram()

ggplot(state_data, aes(x = log(area))) + geom_histogram()
```



```{r}
par(nfrow = c(2,3))

pop <- ggplot(state_data, aes(y = population)) + geom_boxplot()
income <- ggplot(state_data, aes(y = income)) + geom_boxplot() 
illiteracy <- ggplot(state_data, aes(y = illiteracy)) + geom_boxplot()  
life_exp <- ggplot(state_data, aes(y = life_exp)) + geom_boxplot()
murder <- ggplot(state_data, aes(y = murder)) + geom_boxplot()  
grad <- ggplot(state_data, aes(y = hs_grad)) + geom_boxplot()  
frost <- ggplot(state_data, aes(y = frost)) + geom_boxplot()
area <- ggplot(state_data, aes(y = area)) + geom_boxplot()

pop + income + illiteracy + life_exp + murder + grad + frost + area 
               
```

Life expectancy, and murder seem to have a relationship, as well as life expectancy and high school grad and life expectancy and illiteracy. There seems to be a slight relationship between life expectancy and frost. We ended up transforming `population`, `illiteracy` and `area` because they looked skewed when plotted. 

```{r}
state_data <- 
  state_data %>% 
  mutate(population = log(population), 
         illiteracy = 1/(log(illiteracy + 1)), 
         area = log(area))
```

c) 

```{r}
full_model <- lm(life_exp ~ ., data = state_data)
only_1 <- lm(life_exp ~ 1, data = state_data)
summary(full_model)


forward_model <- step(only_1, direction = "forward", scope = formula(full_model))
summary(forward_model)

backward_model <- step(full_model, direction = "backward")

summary(backward_model)

both_model <- step(full_model, direction = "both")

summary(both_model)

model_nofrost <- lm(life_exp ~ population + murder + hs_grad + frost, data = state_data)

summary(model_nofrost)
```
Doing backward, both, and regular subsetting gets the same result to include population, murder, hs_grad, and frost, area. 
`life_exp` = `population`$\beta_1$ + `murder` $\beta_2$ + `hs_grad` $\beta_3$ + `frost` $\beta_4$ 

The only "close call" variable is `frost`, but the adjusted r-squared value decreased when you take `frost` out, so I chose to keep it in the model. 

```{r}
pairs(state_data)
corrplot(cor(state_data), type = "upper", diag = FALSE)
```

There is a strong relationship between `illiteracy` and `hs_grad`. Our subset only contains `hs_grad` and not `illiteracy`. 

d) 

```{r}
subsets = regsubsets(life_exp ~., data = state_data)
subset_res = summary(subsets)

subset_res
plot(subset_res$cp, xlab = "No. of Parameters", ylab = "CP Statistic")
plot(subset_res$adjr2, xlab = "No. of Parameters", ylab = "Adj R2")
```

From the regular subsetting, we can see that 4 parameters is the best model, which is what we got from forward, backward, and a combination of both in subsetting for a model. 

e) 

```{r}
data <- state_data %>% select(-life_exp)
set.seed(5)
lambda_seq <- 10^seq(-3,0, by = 0.1)
cv_object <- cv.glmnet(as.matrix(data[1:7]), state_data$life_exp, lambda = lambda_seq, nfolds = 5)

cv_object
tibble(lambda = cv_object$lambda, 
       mean_cv_error = cv_object$cvm) %>% 
  ggplot(aes(x = lambda, y = mean_cv_error)) + geom_point()

cv_object$lambda.min

fit_bestcv <- glmnet(as.matrix(data[1:7]), state_data$life_exp, lambda = cv_object$lambda.min)
coef(fit_bestcv)
```

The best lambda for our model is `r cv_object$lambda.min`. 

f) 

```{r}

plot(forward_model)

plot(backward_model)

plot(both_model)

y_predict <- predict(cv_object, as.matrix(state_data[-4]), s = "lambda.min")
residuals <- state_data$life_exp - y_predict
plot(y_predict, residuals)

```


```{r}
train <- trainControl(method = "cv", number = 10)
 
model_caret = train(life_exp ~ population + murder + hs_grad + frost, 
                    data = state_data, trControl = train, method = 'lm', 
                    na.action = na.pass)

model_caret$finalModel

print(model_caret)
```

Each of the models seem to have constant variance, are approximately normal and variance seems to be normal. 

e) 

Looking at a bunch of different models, we recommend using a model that includes `population` `murder` `hs_grad` and `frost`. This was the one that was recommended to use by regular subsetting, which picks a model based on the adjusted $r^2$, which will only increase if you add a parameter that is helpful. Additionally, when we did forward, backward, and a combination of both subsetting, we got that the parameters should be 4. We also did lasso, which helps to fit a model when there are multicollinearities in the model. We found in this as well that 4 parameters should be included in the model.  The RMSE is 0.79, which means that our model is off by less than 1 year at predicting `life_expectancy`. The RMSE tells us how good the model is at predicting values in a data set, so having a smaller value is best. 
